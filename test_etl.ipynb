{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoOptionError",
     "evalue": "No option 'user' in section: 'mysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\configparser.py:789\u001b[0m, in \u001b[0;36mRawConfigParser.get\u001b[1;34m(self, section, option, raw, vars, fallback)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 789\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43moption\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\collections\\__init__.py:941\u001b[0m, in \u001b[0;36mChainMap.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__missing__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\collections\\__init__.py:933\u001b[0m, in \u001b[0;36mChainMap.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__missing__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNoOptionError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m mysql_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarehouse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m mysql_url \u001b[38;5;241m=\u001b[39m my_conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmysql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjdbc_url\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m mysql_db\n\u001b[0;32m     29\u001b[0m mysql_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmy_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmysql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: my_conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmysql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: my_conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmysql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m }\n\u001b[0;32m     35\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETLApp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mysql-connector-j-9.0.0.jar\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.cassandra.auth.password\u001b[39m\u001b[38;5;124m\"\u001b[39m, my_conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcassandra\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m)) \\\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_latest_time_cassandra\u001b[39m():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\configparser.py:792\u001b[0m, in \u001b[0;36mRawConfigParser.get\u001b[1;34m(self, section, option, raw, vars, fallback)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fallback \u001b[38;5;129;01mis\u001b[39;00m _UNSET:\n\u001b[1;32m--> 792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoOptionError(option, section)\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fallback\n",
      "\u001b[1;31mNoOptionError\u001b[0m: No option 'user' in section: 'mysql'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "from uuid import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from uuid import * \n",
    "from uuid import UUID\n",
    "import time_uuid \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.window import Window as W\n",
    "import configparser\n",
    "\n",
    "my_conf = configparser.ConfigParser()\n",
    "my_conf.read('./config/config.ini') \n",
    "\n",
    "mysql_db = 'warehouse'\n",
    "mysql_url = my_conf.get('mysql', 'jdbc_url') + '/' + mysql_db\n",
    "mysql_properties = {\n",
    "    \"user\": my_conf.get('mysql', 'user'),\n",
    "    \"password\": my_conf.get('mysql', 'password'),\n",
    "    \"driver\": my_conf.get('mysql', 'driver')\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETLApp\") \\\n",
    "    .config(\"spark.jars\", \"./mysql-connector-j-9.0.0.jar\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.4.1\") \\\n",
    "    .config(\"spark.cassandra.auth.username\", my_conf.get('cassandra', 'username')) \\\n",
    "    .config(\"spark.cassandra.auth.password\", my_conf.get('cassandra', 'password')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_latest_time_cassandra():\n",
    "    data = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table = 'tracking',keyspace = 'logs').load()\n",
    "    cassandra_latest_time = data.agg({'ts':'max'}).take(1)[0][0]\n",
    "    return cassandra_latest_time\n",
    "\n",
    "def get_mysql_latest_time(mysql_url, mysql_properties):    \n",
    "    sql = \"\"\"(select max(latest_modified_time) from events_etl) data\"\"\"\n",
    "    mysql_time = spark.read.format('jdbc').options(url=mysql_url, properties=mysql_properties, dbtable=sql).load()\n",
    "    mysql_time = mysql_time.take(1)[0][0]\n",
    "    if mysql_time is None:\n",
    "        mysql_latest = '1998-01-01 23:59:59'\n",
    "    else :\n",
    "        mysql_latest = mysql_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return mysql_latest \n",
    "\n",
    "def calculating_clicks(df):\n",
    "    clicks_data = df.filter(df.custom_track == 'click')\n",
    "    clicks_data = clicks_data.na.fill({'bid':0})\n",
    "    clicks_data = clicks_data.na.fill({'job_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'publisher_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'group_id':0})\n",
    "    clicks_data = clicks_data.na.fill({'campaign_id':0})\n",
    "    clicks_data.registerTempTable('clicks')\n",
    "    clicks_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , avg(bid) as bid_set, count(*) as clicks , sum(bid) as spend_hour from clicks\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return clicks_output \n",
    "    \n",
    "def calculating_conversion(df):\n",
    "    conversion_data = df.filter(df.custom_track == 'conversion')\n",
    "    conversion_data = conversion_data.na.fill({'job_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'publisher_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'group_id':0})\n",
    "    conversion_data = conversion_data.na.fill({'campaign_id':0})\n",
    "    conversion_data.registerTempTable('conversion')\n",
    "    conversion_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as conversions  from conversion\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return conversion_output \n",
    "    \n",
    "def calculating_qualified(df):    \n",
    "    qualified_data = df.filter(df.custom_track == 'qualified')\n",
    "    qualified_data = qualified_data.na.fill({'job_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'publisher_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'group_id':0})\n",
    "    qualified_data = qualified_data.na.fill({'campaign_id':0})\n",
    "    qualified_data.registerTempTable('qualified')\n",
    "    qualified_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as qualified  from qualified\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return qualified_output\n",
    "    \n",
    "def calculating_unqualified(df):\n",
    "    unqualified_data = df.filter(df.custom_track == 'unqualified')\n",
    "    unqualified_data = unqualified_data.na.fill({'job_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'publisher_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'group_id':0})\n",
    "    unqualified_data = unqualified_data.na.fill({'campaign_id':0})\n",
    "    unqualified_data.registerTempTable('unqualified')\n",
    "    unqualified_output = spark.sql(\"\"\"select job_id , date(ts) as date , hour(ts) as hour , publisher_id , campaign_id , group_id , count(*) as unqualified  from unqualified\n",
    "    group by job_id , date(ts) , hour(ts) , publisher_id , campaign_id , group_id \"\"\")\n",
    "    return unqualified_output\n",
    "    \n",
    "def process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output):\n",
    "    final_data = clicks_output.join(conversion_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full').\\\n",
    "    join(qualified_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full').\\\n",
    "    join(unqualified_output,['job_id','date','hour','publisher_id','campaign_id','group_id'],'full')\n",
    "    return final_data \n",
    "    \n",
    "def process_cassandra_data(df):\n",
    "    clicks_output = calculating_clicks(df)\n",
    "    conversion_output = calculating_conversion(df)\n",
    "    qualified_output = calculating_qualified(df)\n",
    "    unqualified_output = calculating_unqualified(df)\n",
    "    final_data = process_final_data(clicks_output,conversion_output,qualified_output,unqualified_output)\n",
    "    return final_data\n",
    "\n",
    "def retrieve_company_data(mysql_url, mysql_properties):\n",
    "    sql = \"\"\"(SELECT id as job_id, company_id, group_id, campaign_id FROM job) test\"\"\"\n",
    "    company = spark.read.format('jdbc').options(url=mysql_url, dbtable=sql, properties=mysql_properties).load()\n",
    "    return company \n",
    "    \n",
    "def import_to_mysql(output):\n",
    "    final_output = output.select('job_id','date','hour','publisher_id','company_id','campaign_id','group_id','unqualified','qualified','conversions','clicks','bid_set','spend_hour')\n",
    "    final_output = final_output.withColumnRenamed('date','dates').withColumnRenamed('hour','hours').withColumnRenamed('qualified','qualified_application').\\\n",
    "    withColumnRenamed('unqualified','disqualified_application').withColumnRenamed('conversions','conversion')\n",
    "    final_output = final_output.withColumn('sources',lit('Cassandra'))\n",
    "    final_output.printSchema()\n",
    "    final_output.write.format(\"jdbc\") \\\n",
    "    .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/warehouse\") \\\n",
    "    .option(\"dbtable\", \"events_etl\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .save()\n",
    "    return print('Data imported successfully')\n",
    "\n",
    "def main_task(mysql_time):\n",
    "    host = 'localhost'\n",
    "    port = '3306'\n",
    "    db_name = 'warehouse'\n",
    "    user = 'root'\n",
    "    password = 'root'\n",
    "    url = 'jdbc:mysql://' + host + ':' + port + '/' + db_name\n",
    "    driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "    print('The host is ' ,host)\n",
    "    print('The port using is ',port)\n",
    "    print('The db using is ',db_name)\n",
    "    print('-----------------------------')\n",
    "    print('Retrieving data from Cassandra')\n",
    "    print('-----------------------------')\n",
    "    df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"tracking\",keyspace=\"logs\").load().where(col('ts')>= mysql_time)\n",
    "    print('-----------------------------')\n",
    "    print('Selecting data from Cassandra')\n",
    "    print('-----------------------------')\n",
    "    df = df.select('ts','job_id','custom_track','bid','campaign_id','group_id','publisher_id')\n",
    "    df = df.filter(df.job_id.isNotNull())\n",
    "    df.printSchema()\n",
    "#   process_df = process_df(df)\n",
    "    print('-----------------------------')\n",
    "    print('Processing Cassandra Output')\n",
    "    print('-----------------------------')\n",
    "    cassandra_output = process_cassandra_data(df)\n",
    "    print('-----------------------------')\n",
    "    print('Merge Company Data')\n",
    "    print('-----------------------------')\n",
    "    company = retrieve_company_data(mysql_url, mysql_properties)\n",
    "    print('-----------------------------')\n",
    "    print('Finalizing Output')\n",
    "    print('-----------------------------')\n",
    "    final_output = cassandra_output.join(company,'job_id','left').drop(company.group_id).drop(company.campaign_id)\n",
    "    print('-----------------------------')\n",
    "    print('Import Output to MySQL')\n",
    "    print('-----------------------------')\n",
    "    import_to_mysql(final_output)\n",
    "    return print('Task Finished')\n",
    "\n",
    "while True :\n",
    "    start_time = datetime.datetime.now()\n",
    "    cassandra_time = get_latest_time_cassandra()\n",
    "    print('Cassandra latest time is {}'.format(cassandra_time))\n",
    "    mysql_time = get_mysql_latest_time(mysql_url, mysql_properties)\n",
    "    print('MySQL latest time is {}'.format(mysql_time))\n",
    "    if cassandra_time > mysql_time : \n",
    "        print(\"Do main task\")\n",
    "        main_task(mysql_time)\n",
    "    else :\n",
    "        print(\"No new data found\")\n",
    "    end_time = datetime.datetime.now()\n",
    "    execution_time = (end_time - start_time).total_seconds()\n",
    "    print('Job takes {} seconds to execute'.format(execution_time))\n",
    "    time.sleep(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
